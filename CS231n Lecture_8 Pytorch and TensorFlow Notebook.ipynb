{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS231n Lecture_8 Pytorch and TensorFlow Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running example: Train a two-layer ReLU network on random data with L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhaomin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_group_method():\n",
    "    \"\"\"\n",
    "    The purpose to use group is to avoid \n",
    "    weights copied from disk and keep them in the GPU\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define\n",
    "\n",
    "    ## create placeholder(no compute)\n",
    "    N, D, H = 64, 1000, 100\n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    ## create w1 and w2 as variable, which presists in the graph between calls(no compute)\n",
    "    w1 = tf.Variable(tf.random_normal((D, H)))\n",
    "    w2 = tf.Variable(tf.random_normal((H, D)))\n",
    "\n",
    "    ## build graph \n",
    "    ### forward pass(no compute)\n",
    "    h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "    y_pred = tf.matmul(h, w2)\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "    ### tell tf to compute loss of gradient with respect to w1 and w2(no compute)\n",
    "    grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "    ### add assign operations to update w1 and w2 as part of the graph.\n",
    "    learning_rate = 2.7e-5\n",
    "    new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "    new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "    ### add dummy graph node that depends on updates\n",
    "    updates = tf.group(new_w1, new_w2)\n",
    "\n",
    "    # Run\n",
    "    loss_log = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        values = {x: np.random.randn(N, D),\n",
    "                  y: np.random.randn(N, D),}\n",
    "        for t in range(50):\n",
    "            # out = sess.run([loss, grad_w1, grad_w2],\n",
    "            #                feed_dict=values)\n",
    "            # loss_val, grad_w1_val, grad_w2_val = out\n",
    "\n",
    "            ##the updates here tell graph to compute dummy node\n",
    "            loss_val, _ = sess.run([loss, updates], feed_dict=values)\n",
    "            loss_log.append(loss_val)\n",
    "    return loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_optimizer():    \n",
    "    \n",
    "    \n",
    "    # Define\n",
    "\n",
    "    N, D, H = 64, 1000, 100\n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "\n",
    "    w1 = tf.Variable(tf.random_normal((D, H)))\n",
    "    w2 = tf.Variable(tf.random_normal((H, D)))\n",
    "\n",
    "    # build graph\n",
    "    \n",
    "    # forward pass(no compute)\n",
    "    h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "    y_pred = tf.matmul(h, w2)\n",
    "\n",
    "    loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(2.7e-5)\n",
    "    # variable w1 and w2 are marked as trainable by default here\n",
    "    updates = optimizer.minimize(loss)\n",
    "\n",
    "    # Run\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # tf.global_variables_initializer uses tf.random_normal to initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        values = {x: np.random.randn(N, D),\n",
    "                  y: np.random.randn(N, D),}\n",
    "        loss_log = []\n",
    "        for t in range(50):\n",
    "#             out = sess.run([loss, grad_w1, grad_w2],\n",
    "#                            feed_dict=values)\n",
    "#             loss_val, grad_w1_val, grad_w2_val = out\n",
    "\n",
    "            # the updates here tell graph to compute dummy node\n",
    "            loss_val, _ = sess.run([loss, updates], feed_dict=values)\n",
    "            loss_log.append(loss_val)\n",
    "    return loss_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_tf_layers():\n",
    "\n",
    "    N, D, H = 64, 1000, 100\n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "\n",
    "    ##### use tf.layers to form the layers #####\n",
    "    init = tf.contrib.layers.xavier_initializer()\n",
    "    # tf.layers automatically sets up weight and bias.\n",
    "    h = tf.layers.dense(inputs=x, units=H, activation=tf.nn.relu, kernel_initializer=init)\n",
    "    y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)\n",
    "    ############################################\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-1)\n",
    "    updates = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # tf.global_variables_initializer uses tf.random_normal to initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        values = {x: np.random.randn(N, D),\n",
    "                  y: np.random.randn(N, D),}\n",
    "        loss_log = []\n",
    "        for t in range(50):\n",
    "#             out = sess.run([loss, grad_w1, grad_w2],\n",
    "#                            feed_dict=values)\n",
    "#             loss_val, grad_w1_val, grad_w2_val = out\n",
    "\n",
    "            # the updates here tell graph to compute dummy node\n",
    "            loss_val, _ = sess.run([loss, updates], feed_dict=values)\n",
    "            loss_log.append(loss_val)\n",
    "    return loss_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_keras_1():\n",
    "\n",
    "    N, D, H = 64, 1000, 100\n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    \n",
    "    ##### use keras to form the layers #####\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(H, input_shape=(D, ), activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(D))\n",
    "    y_pred = model(x)\n",
    "    ########################################\n",
    "    loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-1)\n",
    "    updates = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # tf.global_variables_initializer uses tf.random_normal to initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        values = {x: np.random.randn(N, D),\n",
    "                  y: np.random.randn(N, D),}\n",
    "        loss_log = []\n",
    "        for t in range(50):\n",
    "            # the updates here tell graph to compute dummy node\n",
    "            loss_val, _ = sess.run([loss, updates], feed_dict=values)\n",
    "            loss_log.append(loss_val)\n",
    "    return loss_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_keras_2():\n",
    "    \n",
    "    N, D, H = 64, 1000, 100\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(H, input_shape=(D, ), activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(D))\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.mean_absolute_error,\n",
    "                  optimizer=tf.keras.optimizers.SGD(lr=1e-1))\n",
    "    \n",
    "    x = np.random.randn(N, D)\n",
    "    y = np.random.randn(N, D)\n",
    "    \n",
    "    history = model.fit(x, y, epochs=50, batch_size=N)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#loss_log = train_with_group_method()\n",
    "#loss_log = train_with_optimizer()\n",
    "#loss_log = train_with_tf_layers()\n",
    "loss_log = train_with_keras_1()\n",
    "\n",
    "x = np.array([i for i in range(50)])\n",
    "plt.plot(x, np.log10(loss_log), 'o')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
